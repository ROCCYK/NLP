{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "jAIhJqx_Foyt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    # Constructor method for the BigramLanguageModel class\n",
        "    def __init__(self):\n",
        "        # Initialize the superclass (nn.Module) constructor\n",
        "        super().__init__()\n",
        "        # Initialize vocabulary, token embeddings, and other parameters and set to None\n",
        "        self.vocab = None\n",
        "        self.token_embeddings_table = None\n",
        "        self.vocab_size = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        # Initialize vocab size and set to int type\n",
        "        self.vocab_size: int\n",
        "        # Set the device to GPU if available, else CPU\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Initialize input length and batch size and set to None\n",
        "        self.input_length = None\n",
        "        self.batch_size = None\n",
        "\n",
        "    # Forward method to define computation at every call\n",
        "    def forward(self, in_ids, target=None):\n",
        "        # Embed input ids using the token embeddings table\n",
        "        in_ids_emb = self.token_embeddings_table(in_ids) # batch_size x vocab_size\n",
        "        # Conditional block for calculating loss\n",
        "        if target is None:\n",
        "            ce_loss = None\n",
        "        else:\n",
        "            batch_size, input_length, vocab_size = in_ids_emb.shape\n",
        "            token_rep = in_ids_emb.view(batch_size * input_length, vocab_size)\n",
        "            targets = target.view(batch_size * input_length)\n",
        "            # Calculate cross-entropy loss\n",
        "            ce_loss = F.cross_entropy(token_rep, targets)\n",
        "        return in_ids_emb, ce_loss\n",
        "\n",
        "    # Fit method for training the model\n",
        "    def fit(self, train_iters=100, eval_iters=10, lr=0.001):\n",
        "        # Initialize Adam optimizer\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        for iteration in range(train_iters):\n",
        "            # Evaluate and print average loss at specified intervals\n",
        "            if iteration % eval_iters == 0:\n",
        "                avg_loss = self.eval_loss(eval_iters)\n",
        "                print(f\"iter {iteration} train {avg_loss['train']} val {avg_loss['eval']}\")\n",
        "            # Get batch of data and compute loss\n",
        "            inputs, targets = self.get_batch(split='train')\n",
        "            _, ce_loss = self(inputs, targets)\n",
        "            # Clear gradients, backpropagate loss, and update model parameters\n",
        "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
        "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
        "            optimizer.step()  # update network parameters w.r.t the loss\n",
        "\n",
        "    # Generate method for text generation\n",
        "    def generate(self, context_tokens, max_new_tokens):\n",
        "        # Loop to generate the specified number of new tokens\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Generate token representations for the current context tokens\n",
        "            token_rep, _ = self(context_tokens)\n",
        "            # Extract the representation of the last token in the sequence\n",
        "            last_token_rep = token_rep[:, -1, :]\n",
        "            # Compute the probabilities of the next token using softmax\n",
        "            probs = F.softmax(last_token_rep, dim=-1)\n",
        "            # Sample the next token based on the probabilities\n",
        "            next_token = torch.multinomial(probs, 1)\n",
        "            # Concatenate the next token to the context tokens\n",
        "            context_tokens = torch.cat([context_tokens, next_token], dim=1)\n",
        "            # Return the context tokens with the newly generated tokens\n",
        "        return context_tokens\n",
        "\n",
        "    # Preprocess text data\n",
        "    def prep(self, text):\n",
        "        # Split text into training and validation sets\n",
        "        n = len(text)\n",
        "        self.train_text = text[:int(n * 0.9)]\n",
        "        self.val_text = text[int(n * 0.9):]\n",
        "\n",
        "        # Convert text to tensors\n",
        "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
        "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
        "\n",
        "        # Initialize token embeddings table\n",
        "        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.vocab_size)\n",
        "\n",
        "    # Create batches of data\n",
        "    def get_batch(self, split='train', input_length=8, batch_size=4):\n",
        "        data = self.train_data if split == 'train' else self.val_data\n",
        "        # Get random chunks of data\n",
        "        ix = torch.randint(len(data) - input_length, (batch_size,))\n",
        "        inputs_batch = torch.stack([data[i:i + input_length] for i in ix])\n",
        "        targets_batch = torch.stack([data[i + 1:i + input_length + 1] for i in ix])\n",
        "        # Move batches to the specified device\n",
        "        inputs_batch = inputs_batch.to(self.device)\n",
        "        targets_batch = targets_batch.to(self.device)\n",
        "        return inputs_batch, targets_batch\n",
        "\n",
        "# Sample text for training\n",
        "text = 'a quick brown fox jumps over the lazy dog.\\\\n ' \\\n",
        "       'lazy dog and a quick brown fox.\\\\n' \\\n",
        "       'a dog is lazy and fox is quick.\\\\n' \\\n",
        "       'fox jumps and dog is lazy.\\\\n' \\\n",
        "       'dog is lazy and fox is brown.'\n",
        "\n",
        "# Instantiate the BigramLanguageModel class\n",
        "model = BigramLanguageModel()\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(model.device)\n",
        "# Preprocess the text data for the model\n",
        "model.prep(text)\n",
        "# Filter the model parameters to only include those that require gradients\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# Calculate and print the total number of trainable parameters in the model\n",
        "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
        "# Generate a batch of input and output data for training\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "# Pass the batch through the model (this step is likely for forward pass testing)\n",
        "_, _ = model(input_batch, output_batch)\n",
        "# Train the model with specified number of iterations, evaluation intervals, and learning rate\n",
        "model.fit(train_iters=10000, eval_iters=500, lr=0.001)\n",
        "# Generate text using the model, starting with an initial context of zeros\n",
        "outputs = model.generate(context_tokens=torch.zeros((1, 1), dtype=torch.long,\n",
        "                         device=model.device), max_new_tokens=100)\n",
        "# Print the generated text outputs\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "lflvQE_TITcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}