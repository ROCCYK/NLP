{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "jAIhJqx_Foyt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Look at the very last character to generate next\n",
        "    @Author: Uzair Ahmad\n",
        "    2022\n",
        "'''\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):    # Defines a class that inherits from nn.Module, making it a neural network module in PyTorch.\n",
        "    def __init__(self):    # The __init__ method initializes the instance and declares several instance variables.\n",
        "        super().__init__()\n",
        "        self.vocab = None\n",
        "        self.token_embeddings_table = None\n",
        "        self.vocab_size = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.vocab_size: int\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'   # Utilize a GPU if available, otherwise default to using the CPU.\n",
        "        # input_length = how many consecutive tokens/chars in one input\n",
        "        self.input_length = None\n",
        "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
        "        self.batch_size = None\n",
        "\n",
        "    def forward(self, in_ids, target=None):\n",
        "        in_ids_emb = self.token_embeddings_table(in_ids) # uses an embedding table (`self.token_embeddings_table`) to convert input token IDs (`in_ids`) into dense vector representations, with dimensions `batch_size` x `vocab_size`.\n",
        "        if target is None:    # The function checks if a target tensor is provided; if not, it skips computing the cross entropy loss.\n",
        "            ce_loss = None\n",
        "        else:\n",
        "            # Reshapes the tensor for cross entropy loss calculation, provided a target tensor is available.\n",
        "            batch_size, input_length, vocab_size = in_ids_emb.shape   # Retrieves the dimensions of the input embeddings.\n",
        "            token_rep = in_ids_emb.view(batch_size * input_length, vocab_size)  # Reshapes the input embeddings to merge the batch and input length dimensions, a common step for preparing data for loss computation by treating each token in each sequence of the batch independently.\n",
        "            targets = target.view(batch_size * input_length)  # `in_ids_emb` is being transformed from a three-dimensional tensor to a two-dimensional one, effectively flattening the batch and length dimensions.\n",
        "            ce_loss = F.cross_entropy(token_rep, targets)   # Calculates the loss between the predictions (`token_rep`) and the actual targets (`targets`). This function applies the `log_softmax` function to the input predictions and then computes the negative log likelihood loss.\n",
        "        return in_ids_emb, ce_loss\n",
        "\n",
        "    def fit(self, train_iters=100, eval_iters=10, lr=None):  # Training the language model.\n",
        "        learning_rate = lr if lr is not None else 0.01\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)  #  Initializes an Adam optimizer, to be used for updating the model's weights.\n",
        "        for iteration in range(train_iters):  # Each iteration signifies a single step in the training process.\n",
        "            if iteration % (train_iters // 20) == 0:\n",
        "                avg_loss = self.eval_loss(eval_iters)\n",
        "                print(f\"iter {iter} train {avg_loss['train']} val {avg_loss['eval']}\")\n",
        "            inputs, targets = self.get_batch(split='train')   # Retrieves a batch of input-target pairs from the training data, where the input is a sequence of token IDs and the target is the sequence of corresponding next tokens.\n",
        "            _, ce_loss = self(inputs, targets)\n",
        "            optimizer.zero_grad(set_to_none=True)  # Clears gradients from the previous step.\n",
        "            ce_loss.backward()  # Backpropagates the loss to each unit in the network.\n",
        "            optimizer.step()  # Updates network parameters with respect to the computed loss.\n",
        "\n",
        "    def generate(self, context_tokens, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):  # During each iteration, the model predicts the next token using the current `context_tokens`. The `max_new_tokens` parameter specifies the maximum number of new tokens the function will generate.\n",
        "            token_rep, _ = self(context_tokens)\n",
        "            last_token_rep = token_rep[:, -1, :]   # Retrieves the representation of the last token in the sequence.\n",
        "            probs = F.softmax(last_token_rep, dim=1)  # Applies the softmax function to the representation of the last token, converting the raw model outputs into a probability distribution over all possible next tokens.\n",
        "            next_token = torch.multinomial(probs, num_samples=1)   # Samples a token from the probability distribution, introducing randomness into the text generation process for more diverse and interesting output.\n",
        "            context_tokens = torch.cat((context_tokens, next_token), dim=1)  # Updates `context_tokens` by appending the newly predicted `next_token` to it, which will then be used as the input to the model in the next iteration of the loop.\n",
        "        output_text = self.decoder(context_tokens[0].tolist())   # Converts the sequence of token IDs back into human-readable text.\n",
        "        return output_text\n",
        "\n",
        "    @torch.no_grad()  # Instructs PyTorch to not prepare for back-propagation, typically using a context manager.\n",
        "    def eval_loss(self, eval_iters):\n",
        "        perf = {}\n",
        "        # Before running inference, set dropout and batch normalization layers to evaluation mode.\n",
        "        self.eval()\n",
        "        for split in ['train', 'eval']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                tokens, targets = self.get_batch(split)  # Retrieves a random batch of inputs and targets.\n",
        "                _, ce_loss = self(tokens, targets)  # Performs the forward pass, which computes the model's predictions.\n",
        "                losses[k] = ce_loss.item()  # Obtains the value of the loss tensor as a standard Python number.\n",
        "            perf[split] = losses.mean()\n",
        "        self.train()  # trains the model reccursively\n",
        "        return perf\n",
        "\n",
        "    def prep(self, text):\n",
        "        self.vocab = sorted(list(set(text)))  # Generates a vocabulary by identifying all unique characters in the input text, converting them into a list, and then sorting the list.\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        ctoi = {c: i for i, c in  # Constructs a dictionary that associates each character (c) in the vocabulary with a unique integer (i). This mapping is essential because neural networks require input tokens to be represented as numerical values, as they cannot directly process textual data.\n",
        "                enumerate(self.vocab)}  # Establishes a mapping from characters (c) to integers (i), creating a character-to-integer (c-to-i) mapping. It assigns a unique integer value (i) to each character in the vocabulary.\n",
        "        itoc = {i: c for c, i in ctoi.items()}  # Creates a mapping from integers (i) to characters (c), forming an integer-to-character (i-to-c) mapping. It allows for the conversion of integers back to their corresponding characters in the vocabulary.\n",
        "\n",
        "        self.encoder = lambda text: [ctoi[c] for c in text]  # Converts a sequence of characters into a sequence of corresponding indices using the character-to-integer (ctoi) mapping. It assigns an index to each character in the input sequence.\n",
        "        self.decoder = lambda nums: ''.join([itoc[i] for i in nums])  #  Conducts the reverse operation, transforming a sequence of indices back into a sequence of characters using the integer-to-character (itoc) mapping. It allows for the reconstruction of the original character sequence from the indices.\n",
        "\n",
        "        n = len(text)\n",
        "        self.train_text = text[:int(n * 0.9)]\n",
        "        self.val_text = text[int(n * 0.9):]\n",
        "\n",
        "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
        "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
        "\n",
        "        # The model will convert each input token into a vector of size `vocab_size`.\n",
        "        self.token_embeddings_table = \\\n",
        "            nn.Embedding(self.vocab_size, self.vocab_size)\n",
        "\n",
        "    def get_batch(self, split='train', input_length=8, batch_size=4): # Responsible for generating data batches for model training or evaluation.\n",
        "        data = self.train_data if split == 'train' else self.val_data\n",
        "        # Retrieve random chunks of data with a length of `batch_size`.\n",
        "        ix = torch.randint(len(data) - input_length, (batch_size,)) # Generates `batch_size` random starting points (`ix`) for sequences in the batch, where each sequence has a length of `input_length`.\n",
        "        inputs_batch = torch.stack([data[i:i + input_length] for i in ix])  # It is generated by slicing the data from each starting index.\n",
        "        targets_batch = torch.stack([data[i + 1:i + input_length + 1] for i in ix])  # It is created in a similar manner, with each sequence starting one character after the corresponding sequence in `inputs_batch` and ending one character later.\n",
        "        inputs_batch = inputs_batch.to(self.device)   # Ensures that both the input and target batches are transferred to the appropriate device, which can be either CPU or GPU.\n",
        "        targets_batch = targets_batch.to(self.device) # Ensures that both the input and target batches are transferred to the appropriate device, which can be either CPU or GPU.\n",
        "        return inputs_batch, targets_batch"
      ],
      "metadata": {
        "id": "lflvQE_TITcy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Sample text for training\n",
        "text = 'a quick brown fox jumps of the lazy dog.\\n ' \\\n",
        "       'a quick brown fox jumps of the lazy dog.\\n' \\\n",
        "       'a quick brown fox jumps of the lazy dog.\\n' \\\n",
        "       'a quick brown fox jumps of the lazy dog.\\n' \\\n",
        "       'a quick brown fox jumps of the lazy dog.'\n",
        "\n",
        "# Instantiate the BigramLanguageModel class\n",
        "model = BigramLanguageModel()\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(model.device)\n",
        "# Preprocess the text data for the model\n",
        "model.prep(text)\n",
        "# Filter the model parameters to only include those that require gradients\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# Calculate and print the total number of trainable parameters in the model\n",
        "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
        "# Generate a batch of input and output data for training\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "# Pass the batch through the model (this step is likely for forward pass testing)\n",
        "_, _ = model(input_batch, output_batch)\n",
        "# Train the model with specified number of iterations, evaluation intervals, and learning rate\n",
        "model.fit(train_iters=10000, eval_iters=500, lr=0.001)\n",
        "# Generate text using the model, starting with an initial context of zeros\n",
        "outputs = model.generate(context_tokens=torch.zeros((1, 1), dtype=torch.long,\n",
        "                         device=model.device), max_new_tokens=100)\n",
        "# Print the generated text outputs\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KvtORmbjgLs",
        "outputId": "772e81c2-cc1f-4153-d568-b0d1dc309a73"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params 784\n",
            "iter <built-in function iter> train 3.7328264713287354 val 3.6603426933288574\n",
            "iter <built-in function iter> train 3.1189944744110107 val 3.0882856845855713\n",
            "iter <built-in function iter> train 2.590837240219116 val 2.565760850906372\n",
            "iter <built-in function iter> train 2.1379659175872803 val 2.13832426071167\n",
            "iter <built-in function iter> train 1.759172797203064 val 1.7684252262115479\n",
            "iter <built-in function iter> train 1.4687942266464233 val 1.5164238214492798\n",
            "iter <built-in function iter> train 1.2422937154769897 val 1.300957202911377\n",
            "iter <built-in function iter> train 1.0767475366592407 val 1.1384706497192383\n",
            "iter <built-in function iter> train 0.9663461446762085 val 1.0236057043075562\n",
            "iter <built-in function iter> train 0.8843268752098083 val 0.9516713619232178\n",
            "iter <built-in function iter> train 0.8184308409690857 val 0.9016019105911255\n",
            "iter <built-in function iter> train 0.7866175174713135 val 0.8649009466171265\n",
            "iter <built-in function iter> train 0.7601588368415833 val 0.8382517099380493\n",
            "iter <built-in function iter> train 0.7384308576583862 val 0.8089299201965332\n",
            "iter <built-in function iter> train 0.72303706407547 val 0.7939233779907227\n",
            "iter <built-in function iter> train 0.7131791710853577 val 0.776553213596344\n",
            "iter <built-in function iter> train 0.7023618221282959 val 0.7672250866889954\n",
            "iter <built-in function iter> train 0.7039012908935547 val 0.7707991003990173\n",
            "iter <built-in function iter> train 0.6891452670097351 val 0.768969714641571\n",
            "iter <built-in function iter> train 0.6961244940757751 val 0.7667611241340637\n",
            "\n",
            " lazy dog.\n",
            "a lazy qumps the dof f the by dox juick lazy own a the brown la ox qumps ox f dofof lazy \n"
          ]
        }
      ]
    }
  ]
}